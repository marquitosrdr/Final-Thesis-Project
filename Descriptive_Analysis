---
title: "**Deep Learning Models in Predicting Loan Default Probability**"
subtitle: "**Descriptive Analysis**"
documentclass: scrartcl
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: 
 - name: Marcos Rusinol de Rueda
   affiliation: University of Barcelona & Polytechnic University of Catalonia
   
abstract: ""
keywords: ""
output:
  html_document:
    toc: yes
    toc_depth: 4
    number_section: true
---


```{r global options, include=F}
knitr::opts_chunk$set(echo = F, warnings = F, message = F, fig.align = "H")
options(scipen=999, xtable.comment = FALSE)
```

# Required Packages

```{r}
require(dplyr)
require(readr)
require(ggplot2)
require(tidyverse)
require(colorspace)
require(maps)
require(ggmap)
```

<!--
-->

# Descriptive Analysis

```{r include=FALSE}
dataset <- read.table("datasetclean.csv",header=T, sep=";",colClasses = c("factor","factor","factor","factor","numeric","factor","factor","factor","factor","numeric","factor","factor"))
head(dataset)
```

Once I have cleaned the dataset, it is time to see how the variables are distributed. The objective is to check that all make sense and change the variables formats (factor, numeric, etc) to make them easier to use.

I will start analyzing the numerical variables of the dataset, which are:

1. DisbursementGross
2. NoEmp

For these two variables I will do an histogram and a boxplot.

Then, I will analyize some of the categorical variables of the dataset, which are:

1. State
2. BankState
3. NAICS
4. New
5. UrbanRural
6. RevLineCr
7. LowDoc
8. Recession

For these variables I will do a Pie plot and Barplots.

## Quantitative Variables

### Graphical analysis

```{r, echo=FALSE}

# BoxPlot
par(mfrow=c(1,2))
boxplot(dataset$DisbursementGross, main=paste("Boxplot of DisbursementGross"),ylab = "DisbursementGross", ylim = c(0,500000),col = rgb(0.1,0.1,0.7,0.5))
boxplot(dataset$NoEmp, main=paste("Boxplot of NÂº Employees"),ylab = "NÂº Employees", ylim = c(0,23),col =  rgb(0.1,0.1,0.7,0.5))

# Histogram
par(mfrow=c(1,2))
hist(dataset$DisbursementGross, main=paste("Histogram of DisbursementGross"))
hist(dataset$NoEmp, main=paste("Histogram of NoEmp"))
```

## Qualitative Variables

### Graphical analysis

```{r echo=FALSE}
#dataset_cat <- select (dataset, -ID,-State,-BankState)
listOfColors<-rainbow(39)
par(ask=TRUE)

for(k in 1:dim(dataset_cat)[2]){
  print(paste("variable ", k, ":", names(dataset_cat)[k] ))
  if (!is.numeric(dataset_cat[,k])){ 
    frecs<-table(as.factor(dataset_cat[,k]), useNA="ifany")
    proportions<-frecs/n
    pie(frecs, cex=0.6, main=paste("Pie of", names(dataset_cat)[k]))
    barplot(frecs, las=3, cex.names=0.7, main=paste("Barplot of", names(dataset_cat)[k]), col=listOfColors)
    print(paste("Number of modalities: ", length(frecs)))
    print("Frequency table")
    print(frecs)
    print("Relative frequency table (proportions)")
    print(proportions)
    print("Frequency table sorted")
    print(sort(frecs, decreasing=TRUE))
    print("Relative frequency table (proportions) sorted")
    print(sort(proportions, decreasing=TRUE))
   }
}
par(ask=FALSE)
```


## Bivariate Analysis with the Default variable

Once I have examined the numerical and categorical variables of my dataset, it is time to see how these variables are corralated with my response variable, Default.

Therefore, this part aims to study and examine the data in a wider way. I pretend to get an intuition of how the default variable is distributed among the other variables in the dataset.

### Default vs State

Visualization analysis from State and Default variables. The objective is to see which states have more defaults.

In order to do so, I show a graph by mapping the states with their associate default.

```{r include=FALSE}
abbr2state <- function(abbr){
  # This function transforms a vector containing state abbreviations to a vector with the full state name
  ab    <- tolower(c("AL",
             "AK", "AZ", "KS", "UT", "CO", "CT",
             "DE", "FL", "GA", "HI", "ID", "IL",
             "IN", "IA", "AR", "KY", "LA", "ME",
             "MD", "MA", "MI", "MN", "MS", "MO",
             "MT", "NE", "NV", "NH", "NJ", "NM",
             "NY", "NC", "ND", "OH", "OK", "OR",
             "PA", "RI", "SC", "SD", "TN", "TX",
             "CA", "VT", "VA", "WA", "WV", "WI",
             "WY", "DC"))
  st    <- c("Alabama",
             "Alaska", "Arizona", "Kansas",
             "Utah", "Colorado", "Connecticut",
             "Delaware", "Florida", "Georgia",
             "Hawaii", "Idaho", "Illinois",
             "Indiana", "Iowa", "Arkansas",
             "Kentucky", "Louisiana", "Maine",
             "Maryland", "Massachusetts", "Michigan",
             "Minnesota", "Mississippi", "Missouri",
             "Montana", "Nebraska", "Nevada",
             "New Hampshire", "New Jersey", "New Mexico",
             "New York", "North Carolina", "North Dakota",
             "Ohio", "Oklahoma", "Oregon",
             "Pennsylvania", "Rhode Island", "South Carolina",
             "South Dakota", "Tennessee", "Texas",
             "California", "Vermont", "Virginia",
             "Washington", "West Virginia", "Wisconsin",
             "Wyoming", "District of Columbia")
  st[match(tolower(abbr), ab)]
}
```


```{r echo=FALSE}
dataset1 <- dataset
dataset1$State <- abbr2state(dataset1$State)
dataset1$region <- tolower(dataset1$State)
dataset1$Default <- as.numeric(levels(dataset1$Default))[dataset1$Default]

states <- map_data("state")
map.df <- merge(states,dataset1[sample(nrow(dataset1), 10000),1:13], by = "region", all.x=T)
map.df <- map.df[order(map.df$order),]
ggplot(map.df, aes(x=long,y=lat,group=group))+
  geom_polygon(aes(fill=Default))+
  geom_path()+ 
  scale_fill_gradientn(colours=rev(heat.colors(10)),na.value="grey90")+
  coord_map()
```

**Interpretation:**

Thanks to this plot I can see the states which seem to have more default cases. However, it has to be said that due to the lack of memeory of the computer I just can run 30000 samples (chosen randomly). That is why I do corroborate this results by doing a frequency table, as follows:

```{r echo=FALSE}
svd <- table(dataset1$State)
svd <- as.data.frame(svd)
colnames(svd)[colnames(svd) == 'Var1'] <- "States"
svd <- svd[with(svd,order(-Freq)),]
svd[1:10,]
```

### Default vs DisbursementGross

As the article from where this dataset is extracted says (poner bibligrafia): Gross disbursement is an indicator that seems to be a key risk factor to consider in modelling the default event. The rationale behind is that the larger the loan size, the more likely the underlying business will be established and expanding (i.e., purchasing assets that have some resale value), thereby increasing the likelihood of paying off the loan. This rationale is confirmed by looking at the means and quantiles shown in the following statistics:

```{r echo=FALSE}
# Default event
mean (dataset$DisbursementGross[dataset$Default == 1])
quantile(dataset$DisbursementGross[dataset$Default == 1])

# No default event
mean (dataset$DisbursementGross[dataset$Default == 0])
quantile(dataset$DisbursementGross[dataset$Default == 0])

table <- aggregate(dataset["DisbursementGross"],dataset["Default"],mean)
names(table)[2] <- "DisbursementGross_mean"
table

# Histogram
require(ggplot2)
ggplot(dataset,aes(x=DisbursementGross))+geom_histogram()+facet_grid(~Default)+theme_bw()
```
