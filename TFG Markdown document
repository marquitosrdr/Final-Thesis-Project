---
title: "**Deep Learning Models in Predicting Loan Default Probability**"
date: '`r format(Sys.time(), "%B %d, %Y")`'
author: 
 - name: Marcos Rusiñol de Rueda
   affiliation: University of Barcelona & Polytechnic University of Catalonia
   
abstract: ""
keywords: ""
output:
  html_document:
    toc: yes
    toc_depth: 4
    number_section: true
---


```{r global options, include=F}
knitr::opts_chunk$set(echo = F, warnings = F, message = F, fig.align = "H")
options(scipen=999, xtable.comment = FALSE)
```


# Introduction

## Goals and Motivation

## Structure

# Statistics and Economy: Credit Risk Models

## Credit Risk Models

## Machine Learning in Economics

# Deep Learning

## Rol Deep Learning

## Multilayer Perceptron

## Funciones Activación

## Función Loss (Coste)

## Algoritmos fundamentales DL

## Principales arquitecturas DL

## Overfitting and Underfitting problems and solutions

# Real Case Approach

## Deep Learning Models in Predicting Loan Default Probability

### Requirements

### Keras (Sequential models vs Functional API)

### Others (CPU, GPU)

## Benchmark: Simple Neural Network Algorithm to predict loan default (without considering time-line)

### Functional document

### Dataset background

### Dataset technical features

### Managing the data

<!-- Required Packages -->
```{r include=FALSE}
require(dplyr)
require(readr)
require(ggplot2)
require(tidyverse)
require(colorspace)
require(maps)
require(ggmap)
```

<!--
-->
#### Dataset modifications / Cleaning data

For a deep knowledge of this part, refer to my Github account (https://github.com/marquitosrdr/Final-Thesis-Project).

A summary of what I have done in this part is:

* First, as described above, I create three more variables (New, Default and Recession).

* Second, I deal with the outliers and missing values. In this part I finally chose to use **two numeric variables** (DisbursementGross and NoEmp); and **8 categorical variables**(State, BankState, NAICS, New, UrbanRural, RevLineCr, LowDoc and Recession).

* Third, I decide to remove all the missing values and unuseful variables. That makes my dataset pass from having 899164 rows and 20 variables to having 343467 rows and 12 variables.

#### Descriptive Analysis

For a deep knowledge of this part, refer to my Github account (https://github.com/marquitosrdr/Final-Thesis-Project).

Once I have cleaned the dataset, it is time to see how the variables are distributed. The objective is to check that all make sense and change the variables formats (factor, numeric, etc) to make them easier to use.

I start doing an histogram and a boxplot for the numeric variables

Then, I do a Pie plot and Barplots for some of the categorical variables

Once I have examined the numerical and categorical variables of my dataset, it is time to see how these variables are corralated with my response variable, Default.

Therefore, this part aims to study and examine the data in a wider way. I pretend to get an intuition of how the default variable is distributed among the other variables in the dataset.

I start by doing a graphical analysis from the State and Default variables. The objective is to see which states have more defaults. In order to do so, I show a graph by mapping the states with their associate default.


#### Creating WOE (poner referencia bibliografica de la AWC)


## Deep Leaning NN Model (with stochastic gradient descent)

### Train, Test, Validation sets


### Results and Performance


### Improvements: Models, Regularization, Interpretability, etc.


## Improvements of the benchmark. Trying to implement another version of the DL NN model tested before (overfitting problems --- regularization!)


# Conclusions

## Thesis Extension


# Index of Figures and Tables

# Annexes

# References
