---
title: "**Deep Learning Models in Predicting Loan Default Probability**"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: 
 - name: Marcos Rusiñol de Rueda
   affiliation: University of Barcelona & Polytechnic University of Catalonia
   
abstract: ""
keywords: ""
output:
  pdf_document:
    toc: yes
    toc_depth: 4
    number_section: true
    latex_engine: xelatex
    pandoc_args: [
      "-V", "classoption=twocolumn"
    ]
    template: modified.tex
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
bibliography: master.bibtex
biblio-style: apsr
---


```{r global options, include=F}
knitr::opts_chunk$set(echo = F, warnings = F, message = F, fig.align = "H")
options(scipen=999, xtable.comment = FALSE)
```

# Required Packages

```{r}
require(dplyr)
require(readr)
require(ggplot2)
require(tidyverse)
require(colorspace)
require(maps)
require(ggmap)
```


<!--
# 0. Dataset modifications

The objective of this part is to adapt the dataset variables to the ones I think will be more adequate for the analysis I will do.

Therefore, as described in the thesis, I create three more variables, which are mainly tranformations from the ones I already have.

For more detail about these transformations, please refer to the thesis.

```{r}
dataset <- read_delim("DatosSBA.csv", ";",escape_double = FALSE, col_types = cols(DisbursementDate = col_date(format = "%d-%b-%y"),DisbursementGross = col_number(),BalanceGross = col_number(),ChgOffPrinGr = col_number(),GrAppv = col_number()),trim_ws = TRUE)
head(dataset)
```

```{r}
# Changing variable names to make it easier
colnames(dataset)[colnames(dataset) == 'LoanNr_ChkDgt'] <- "ID"

# First transformation
dataset$NewExist[dataset$NewExist==2] <- 1
dataset$NewExist[dataset$NewExist==1] <- 0
colnames(dataset)[colnames(dataset) == 'NewExist'] <- "New"

# Second transformation
dataset$MIS_Status[dataset$MIS_Status=='CHGOFF'] <- 1
dataset$MIS_Status[dataset$MIS_Status=='P I F'] <- 0
colnames(dataset)[colnames(dataset) == 'MIS_Status'] <- "Default"

# Third transformation
Daysterm <- (dataset$Term)*30
temp_var <- (as.Date(dataset$DisbursementDate))+Daysterm

dataset["Recession"] <- temp_var

dataset<- within(dataset,Recession <- ifelse((dataset$Recession >= ("2007-12-01") & dataset$Recession <= ("2009-06-30")),1,0))

#Removing variables I will not use anymore
dataset <- select(dataset, -Term, -DisbursementDate)

write.table(dataset, file = "dataset.csv", sep = ";", na = "NA", dec = ".", row.names = FALSE, col.names = TRUE)
```
-->


# 1. Cleaning data

```{r}
dataset <- read.table("dataset.csv",header=T, sep=";")
head(dataset)
```

As defined in the thesis, the dataset I am going to use contains 20 columns and 899164 rows.
Before starting it is a good practice to treat this dataset.

Therefore, I will start doing some descriptive analysis, visualizing the variables and treating missing values.

```{r}
# Entire dataset
class(dataset)
dim(dataset)
n<-dim(dataset)[1]
K<-dim(dataset)[2]
names(dataset)
summary(dataset)
## Estructura de la base de datos
str(dataset)
sapply(dataset, class)

# 1st: Treatment of the response variable (Default)
summary(dataset$Default)
# Convert to factor
dataset$Default <- as.factor(dataset$Default)
summary(dataset$Default)

# 2nd: Treatment of numeric variables
summary(dataset$DisbursementGross)
summary(dataset$BalanceGross)
summary(dataset$ChgOffPrinGr)
summary(dataset$GrAppv)

par(mfrow=c(1,2))
boxplot(dataset$DisbursementGross, main=paste("Boxplot of", colnames(dataset)[15]))
boxplot(dataset$BalanceGross, main=paste("Boxplot of", colnames(dataset)[16]))
boxplot(dataset$ChgOffPrinGr, main=paste("Boxplot of", colnames(dataset)[18]))
boxplot(dataset$GrAppv, main=paste("Boxplot of", colnames(dataset)[19]))

# It seems that the variables BalanceGross and ChgOffPrinGr have weird values


# CATEGORITZAR
# Categoric variables
class(dataset[,1])
sapply(dataset, class)

## Missing data
#rowSums(is.na(dataset)) #Nº of missing data per row
colSums(is.na(dataset)) #Nº of missing data per column

# Since I have a lot of data and not too much missing data, I just choose to delete those NA from my data.

dataset <- na.omit(dataset)
#write.table(dataset, file = "datasetclean.csv", sep = ";", na = "NA", dec = ".", row.names = FALSE, col.names = TRUE)

```


The response variable has `r summary(dataset$Default) [1]` and `r summary(dataset$Default) [2]`; also contains `r summary(dataset$Default) [3]`. I am not going to change anything.

By doing the analysis to the numeric variables I notice that `r colnames(dataset)[16]` and ` r colnames(dataset)[18]` have weird values. 

I finally create a clean dataset with which I can start the analysis.

# 2 Descriptive Analysis

```{r}
dataset <- read.table("datasetclean.csv",header=T, sep=";")
head(dataset)
```

Once I have cleaned the dataset, it is time to see how the variables are distributed. The objective is to check that all make sense and change the variable´s formats (factor, numeric, etc) to make them easier to use.

I will start analyzing the numerical variables of the dataset, which are:

1.`r colnames(dataset)[15]`
2.`r colnames(dataset)[16]`
3.`r colnames(dataset)[18]`
4.`r colnames(dataset)[19]`

For these four variables I will do an histrogram and a boxplot.

Then, I will analyize some of the categorical variables of the dataset, which are:



For these variables I will do barplots.

## 2.1 Numerical Variables

### 2.1.1 Graphical analysis

```{r, echo=FALSE}

# BoxPlot

par(mfrow=c(1,2))
boxplot(dataset$DisbursementGross, main=paste("Boxplot of", colnames(dataset)[15]))
boxplot(dataset$BalanceGross, main=paste("Boxplot of", colnames(dataset)[16]))
boxplot(dataset$ChgOffPrinGr, main=paste("Boxplot of", colnames(dataset)[18]))
boxplot(dataset$GrAppv, main=paste("Boxplot of", colnames(dataset)[19]))


# Historgram

par(mfrow=c(1,2))
hist(dataset$DisbursementGross, main=paste("Histogram of", colnames(dataset)[15]))
hist(dataset$BalanceGross, main=paste("Histogram of", colnames(dataset)[16]))
hist(dataset$ChgOffPrinGr, main=paste("Histogram of", colnames(dataset)[18]))
hist(dataset$GrAppv, main=paste("Histogram of", colnames(dataset)[19]))

```


## 2.2 Qualitative Variables

### 2.2.1 Graphical analysis

```{r}
dataset_an<-select(dataset,-ID,-City,-Zip,-Bank,-BankState,-FranchiseCode)

listOfColors<-rainbow(39)
par(ask=TRUE)

for(k in 1:dim(dataset_an)[2]){
  print(paste("variable ", k, ":", names(dataset_an)[k] ))
  if (!is.numeric(dataset_an[,k])){ 
    frecs<-table(as.factor(dataset_an[,k]), useNA="ifany")
    proportions<-frecs/n
    pie(frecs, cex=0.6, main=paste("Pie of", names(dataset_an)[k]))
    barplot(frecs, las=3, cex.names=0.7, main=paste("Barplot of", names(dataset_an)[k]), col=listOfColors)
    print(paste("Number of modalities: ", length(frecs)))
    print("Frequency table")
    print(frecs)
    print("Relative frequency table (proportions)")
    print(proportions)
    print("Frequency table sorted")
    print(sort(frecs, decreasing=TRUE))
    print("Relative frequency table (proportions) sorted")
    print(sort(proportions, decreasing=TRUE))
   }
}
par(ask=FALSE)
```


## 2.3 Bivariate Analysis with the Default variable

Once I have examined the numerical and categorical variables of my dataset, it is time to see how these variables are corralated with my response variable, Default.

Therefore, this part aims to study and examine the data in a wider way. I pretend to get an intuition of how the default variable is distributed among the other variables in the dataset.

### 2.3.1 Default vs State

Visualization analysis from State and Default variables. The objective is to see which states have more defaults.

In order to do so, I show a graph by mapping the states with their associate default.


```{r}
abbr2state <- function(abbr){
  # This function transforms a vector containing state abbreviations to a vector with the full state name
  ab    <- tolower(c("AL",
             "AK", "AZ", "KS", "UT", "CO", "CT",
             "DE", "FL", "GA", "HI", "ID", "IL",
             "IN", "IA", "AR", "KY", "LA", "ME",
             "MD", "MA", "MI", "MN", "MS", "MO",
             "MT", "NE", "NV", "NH", "NJ", "NM",
             "NY", "NC", "ND", "OH", "OK", "OR",
             "PA", "RI", "SC", "SD", "TN", "TX",
             "CA", "VT", "VA", "WA", "WV", "WI",
             "WY", "DC"))
  st    <- c("Alabama",
             "Alaska", "Arizona", "Kansas",
             "Utah", "Colorado", "Connecticut",
             "Delaware", "Florida", "Georgia",
             "Hawaii", "Idaho", "Illinois",
             "Indiana", "Iowa", "Arkansas",
             "Kentucky", "Louisiana", "Maine",
             "Maryland", "Massachusetts", "Michigan",
             "Minnesota", "Mississippi", "Missouri",
             "Montana", "Nebraska", "Nevada",
             "New Hampshire", "New Jersey", "New Mexico",
             "New York", "North Carolina", "North Dakota",
             "Ohio", "Oklahoma", "Oregon",
             "Pennsylvania", "Rhode Island", "South Carolina",
             "South Dakota", "Tennessee", "Texas",
             "California", "Vermont", "Virginia",
             "Washington", "West Virginia", "Wisconsin",
             "Wyoming", "District of Columbia")
  st[match(tolower(abbr), ab)]
}
```


```{r}
dataset1 <- dataset
dataset1$State <- abbr2state(dataset1$State)
dataset1$region <- tolower(dataset1$State)
dataset1$Default <- as.numeric(levels(dataset1$Default))[dataset1$Default]

states <- map_data("state")
map.df <- merge(states,dataset1[sample(nrow(dataset1), 10000),1:21], by = "region", all.x=T)
map.df <- map.df[order(map.df$order),]
ggplot(map.df, aes(x=long,y=lat,group=group))+
  geom_polygon(aes(fill=Default))+
  geom_path()+ 
  scale_fill_gradientn(colours=rev(heat.colors(10)),na.value="grey90")+
  coord_map()
```

*Interpretation:*

We can see that the states which seem to have more default cases are: California, New York, Georgia, Pennsylvania and Ohio.

However, it has to be said that due to the lack of memeory of the computer I just can run 30000 samples (chosen randomly). That is why I do and actually corroborate this results by doing a frequency table, as follows:

```{r}
svd <- table(dataset1$State)
svd <- as.data.frame(svd)
colnames(svd)[colnames(svd) == 'Var1'] <- "States"
svd <- svd[with(svd,order(-Freq)),]
svd[1:10,]
```

Definitely, We can see that the 10 states whith the most default cases are: `r svd[1:10,][1]`

### 2.3.2 Default vs DisbursementGross

As the article from where this dataset is extracted says (poner bibligrafia): Gross disbursement is an indicator that seems to be a key risk factor to consider in modelling the default event. The rationale behind is that the larger the loan size, the more likely the underlying business will be established and expanding (i.e., purchasing assets that have some resale value), thereby increasing the likelihood of paying off the loan. This rationale is confirmed by looking at the quartiles shown in the following statistics:

```{r}
mean (dataset$DisbursementGross[dataset$Default == 1],na.rm = T)
quantile(dataset$DisbursementGross[dataset$Default == 1],na.rm = T)
length(dataset$DisbursementGross[dataset$Default == 1])
sum(is.na(dataset$DisbursementGross[dataset$Default == 1]))


mean (dataset$DisbursementGross[dataset$Default == 0],na.rm = T)
quantile(dataset$DisbursementGross[dataset$Default == 0],na.rm = T)
length(dataset$DisbursementGross[dataset$Default == 0])


aggregate(dataset["DisbursementGross"],dataset["Default"],mean)

```


### 2.3.3 Default vs 


# 3. Creating WOE (poner referencia bibliografica de la AWC)





# 4. Train, Test, Validation sets





